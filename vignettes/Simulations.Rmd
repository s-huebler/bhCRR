---
title: "Simulations"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simulations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 4
)
have_tidy <- requireNamespace("dplyr", quietly = TRUE)&&
  requireNamespace("ggplot2", quietly = TRUE)

if(!have_tidy){
  message("To reproduce the full vignette, please install the suggested packages: dplyr, ggplot2")
}
```

```{r setup}
library(dplyr)
library(ggplot2)
library(simsurv)
#library(bhCRR)
```

# Motivation

Outline a customizable scenario builder so that we can pick and choose which pieces are important.

Every choice that I originally made

-   Design matrix based off of a true dataset

    -   Fixed n, p, distribution (non-zero values, comes from low abundance CLR data)

-   Permutations (for low correlation)

-   Number of competing risks

-   Number of true covariates in each set

    -   Fixed sparsity, masking

-   Effect sizes

    -   Range log HR distribution

    -   Target standard deviation of linear predictors

    -   Masked variables as a multiplicative factor

-   Outcome parametric distribution

    -   Linear form

    -   Weibull ph

        -   Fixed shape and scale

# Specifications

## Helper functions

```{r}

`%||%` <- function(a, b) if (!is.null(a)) a else b

sim_merge_lists <- function(default, user) {
  if (is.null(user)) return(default)
  if (!is.list(user)) stop("Expected a list, got: ", class(user)[1])
  out <- default
  for (nm in names(user)) {
    if (!nm %in% names(out)) {
      out[[nm]] <- user[[nm]]
    } else if (is.list(out[[nm]]) && is.list(user[[nm]])) {
      out[[nm]] <- sim_merge_lists(out[[nm]], user[[nm]])
    } else {
      out[[nm]] <- user[[nm]]
    }
  }
  out
}

```

## List of specification

Things to add:

-   Nonlinear and interaction terms in the truth

```{r}
sim_spec <- function(
  n = 50, # samples
  p = 100, # features
  x = list(), # design matrix
  truth = list(), # variable selection
  risks = list(), # risk 
  censor = list(), #type of censoring
  seed = NULL
  
){
  
  
defaults <- list(
    n = n,
    p = p,
    x = list(
      family = "gaussian",  #  gaussian | binary_latent | zip | mixed
      corr = list(type = "indep", rho = 0.0),  # indep | ar1 | block | factor
      zero_inflation = 0.0,
      standardize = TRUE,

      # optional knobs used by some families/structures
      binary_prob = 0.5,      # for binary_latent
      zip_rate = 1.0,         # for zip (rough first pass)
      factor_k = 5            # for corr$type = "factor"
    ),
    truth = list(
      share_active_set = 0,
      shared_multiplier = 1,
      sparsity = 0.05,
      beta = list(dist = "normal", mean = 0, sd = c(0.3, 0.3)),

      # nonlinear = list(
      #   enabled = FALSE,
      #   n_terms = 0,
      #   forms = c("sin", "square"),
      #   sd = 0.3
      # ),
      # interactions = list(
      #   enabled = FALSE,
      #   n_pairs = 0,
      #   sd = 0.2
      # )
      
      target_eta_sd = c(0.5, 0.5)
    ),
    risks = list(
      cause1 = list(
        baseline = list(dist = "weibull", shape = 1, scale = 0.011),
        link = "PH"
      ),
      cause2 = list(
        baseline = list(dist = "weibull", shape = 1, scale = 0.011),
        link = "PH"
      )
      # cluster = list(
      #   enabled = FALSE,
      #   G = 40,
      #   sd = 0.35,
      #   shared_across_causes = TRUE
      # )
    ),
    censor = list(
      type = "administrative", # none | administrative | random | administrative_plus_random
      administrative_time = 65,
      random = list(dist = "exponential", rate = 0.15)
      # future: target_censor, calibrate, etc.
    ),
    seed = seed
  )

  spec <- defaults
  spec$x <- sim_merge_lists(defaults$x, x)
  spec$truth <- sim_merge_lists(defaults$truth, truth)
  spec$risks <- sim_merge_lists(defaults$risks, risks)
  spec$censor <- sim_merge_lists(defaults$censor, censor)

  class(spec) <- "sim_spec"

  spec
}
```

## Generator wrapper

```{r}
sim_generate <- function(spec,
                         custom_design_matrix = NULL,
                         custom_predictor_set = NULL,
                         custom_betas = NULL,
                         seed = NULL){
  if (!inherits(spec, "sim_spec")) {
    # allow passing a plain list as long as it has the right shape
    class(spec) <- unique(c("sim_spec", class(spec)))
  }
  
  if (!is.null(seed)) {
    set.seed(seed)
  } else if (!is.null(spec$seed)) {
    set.seed(spec$seed)
  }
  
  n <- as.integer(spec$n)
  p <- as.integer(spec$p)
  
  # Design matrix "X"
  ## Allow for custom design matrix
  if(!is.null(custom_design_matrix)){
    X <- data.matrix(custom_design_matrix)
  }else{
    
    X <- sim_generate_x(spec$x, n, p)
  }
  
  print("Success in generating X")
  
  # Variable selection "truth:
  ## Allow for custom covariate choices, but check that custom predictor set is a list of length 2 
  if(!is.null(custom_predictor_set)){
     if(!(typeof(custom_predictor_set) == "list" & 
                length(custom_predictor_set) == 2 &
                sum(custom_predictor_set[[1]]%%1) == 0 &
                sum(custom_predictor_set[[2]]%%1) == 0 &
                max(custom_predictor_set[[1]]) <= p &
                max(custom_predictor_set[[2]]) <= p)){
    warning("custom_predictor_set should be a list of 2 sets of index integers (with max integer <= p). Ignoring custom predictor sets and custom_betas. Generating from truth defaults. ")
    custom_predictor_set <- NULL
    custom_betas <- NULL
    }}
  
    
  if(!is.null(custom_predictor_set)){
    truth$shared_multiplier = spec$truth$shared_multiplier
    truth$cause1 = custom_predictor_set[[1]]
    truth$cause2 = custome_predictor_set[[2]]
    truth$shared_predictors = intersect(custom_predictor_set[[1]],
                                        custom_predictor_set[[2]])
    
    #print real truth rather than defaults
    #number of real shared active sets
    truth$share_active_set = length(truth$shared_predictors)
    #true sparsity based on min sparsity of the 2 causes
    truth$sparsity = min(c(length(truth$cause1), length(truth$cause1)))/p
    #placeholders
    truth$beta = spec$truth$beta
    truth$target_eta_sd = spec$truth$target_eta_sd
  }else{
    truth <- sim_generate_truth(spec$truth, p, seed = seed)
  }
  
  print("Success in generating truth")
  
  
  # Covariate effects (add to "truth")
  ## Allow for custom covariate effects, but check that custom predictors is a list of 
  if(!is.null(custom_betas)){
     if(!(length(custom_betas[[1]]) == length(truth$cause1) &
       length(custom_betas[[2]]) == length(truth$cause2))){
    warning("Lengh of custom betas does not match length of predictor sets. Generating from truth defaults.")
    custom_betas <- NULL
  }}
  ## Covariate effects and linear predictors
  if(!is.null(custom_betas)){
    effects1$betas1 <- custom_betas[[1]]
    effects1$betas1_sd <- sd(effects1$betas1)
    effects1$eta1 <- X[, truth$cause1] %*% custom_betas[[1]]
    effects1$eta1_sd <- sd(effects1$eta1)
    
    effects2$betas2 <- custom_betas[[2]]
    effects2$betas2_sd <- sd(effects2$betas2)
    effects2$eta2 <- X[, truth$cause2] %*% custom_betas[[2]]
    effects2$eta2_sd <- sd(effects2$eta2)
    
    effects <- list(effects1, effects2)
    
    
  }else{
  effects <- sim_compute_effects(X, truth)
  }
  
  if(truth$share_active_set > 0){
      #sort to the shared predictor order
      i1 <- match(truth$shared_predictors, truth$cause1)
      i2 <- match(truth$shared_predictors, truth$cause2)
    truth$shared_multiplier <- effects$effects1$betas1[i1] / effects$effects2$betas2[i2]
    }else{
      truth$shared_multiplier <- NA
    }
  

  truth$beta <- NULL
  truth$target_eta_sd <- NULL
  truth$effects <- effects
  
  #print("Shared predictors")
  #print(truth$shared_predictors)
  
  print("Success in generating truth with outcomes")
  #test_truth <<- truth

  # Outcome "risks"
  ## baseline hazard
  b1 <- spec$risks$cause1$baseline
  b2 <- spec$risks$cause2$baseline

  ## Latent competing event times
  event1 <- sim_weibull_ph(shape = b1$shape %||% 1, 
                       scale = b1$scale %||% 0.011, 
                       eta_val = truth$effects$effects1$eta1)
  
  T1 <- event1$eventtime
  
  event2 <- sim_weibull_ph(shape = b2$shape %||% 1, 
                       scale = b2$scale %||% 0.011, 
                       eta_val = truth$effects$effects2$eta2)
  T2 <- event2$eventtime
  
  print("Success in generating latent event times")

  maxtime <- max(c(T1, T2))
  # Censoring "censor"
  C <- sim_generate_censor_time(spec$censor, n, maxtime)
  time <- pmin(T1, T2, C)

  event1 <- (T1 <= T2) & (T1 <= C)
  event2 <- (T2 <  T1) & (T2 <= C)
  status <- integer(n)
  status[event1] <- 1L
  status[event2] <- 2L
  # remaining are censored (0)
  
  print("Success in generating censoring")

  out <- list(
    id = seq_len(n),
    time = as.numeric(time),
    status = as.integer(status),
    latent_time_event1 = T1,
    latent_time_event2 = T2,
    X = X,
    spec_call = spec,
    truth = truth
  )


  class(out) <- "sim_data"
  out
  
  
}
```

# Design Matrix

-   For future implementation. ChatGPT 5.2 Pro placeholder.

-   Need to add clinical factors as a designation thing

```{r}
sim_generate_x <- function(xspec, n, p) {
  corr_type <- xspec$corr$type %||% "indep"
  rho <- xspec$corr$rho %||% 0

  # 1) latent Gaussian core with requested dependence
  Z <- switch(
    corr_type,
    indep = matrix(stats::rnorm(n * p), nrow = n),
    ar1 = {
      if (!is.finite(rho) || abs(rho) >= 1) stop("For ar1, corr$rho must satisfy abs(rho) < 1.")
      eps <- matrix(stats::rnorm(n * p), nrow = n)
      out <- eps
      if (p >= 2) {
        s <- sqrt(1 - rho^2)
        for (j in 2:p) out[, j] <- rho * out[, j - 1] + s * eps[, j]
      }
      out
    },
    block = {
      block_size <- xspec$corr$block_size %||% 50L
      block_size <- as.integer(block_size)
      if (block_size < 1) stop("For block correlation, corr$block_size must be >= 1.")
      # equicorrelation within blocks via a shared latent factor
      out <- matrix(0, nrow = n, ncol = p)
      nb <- ceiling(p / block_size)
      for (b in seq_len(nb)) {
        j0 <- (b - 1) * block_size + 1
        j1 <- min(b * block_size, p)
        m <- j1 - j0 + 1
        z_shared <- stats::rnorm(n)
        eps <- matrix(stats::rnorm(n * m), nrow = n)
        out[, j0:j1] <- sqrt(max(rho, 0)) * z_shared + sqrt(max(1 - max(rho, 0), 0)) * eps
      }
      out
    },
    factor = {
      k <- as.integer(xspec$factor_k %||% 5L)
      k <- max(1L, k)
      Zf <- matrix(stats::rnorm(n * k), nrow = n)
      Lambda <- matrix(stats::rnorm(k * p), nrow = k) / sqrt(k)
      E <- matrix(stats::rnorm(n * p), nrow = n)
      Zf %*% Lambda + E
    },
    stop("Unknown corr$type: ", corr_type)
  )

  # 2) transform latent Z to requested family
  family <- xspec$family %||% "gaussian"
  X <- switch(
    family,
    gaussian = Z,
    binary_latent = {
      prob <- xspec$binary_prob %||% 0.5
      prob <- min(max(prob, 1e-6), 1 - 1e-6)
      thr <- stats::qnorm(1 - prob)
      (Z > thr) * 1.0
    },
    zip = {
      # Rough first pass: correlated log-intensity from Z
      rate0 <- xspec$zip_rate %||% 1.0
      mu <- log(pmax(rate0, 1e-8)) + 0.3 * Z
      stats::rpois(n * p, lambda = exp(mu)) |> matrix(nrow = n)
    },
    mixed = {
      # first pass: 70% gaussian, 30% binary_latent
      p_bin <- as.integer(round(0.3 * p))
      idx_bin <- if (p_bin > 0) sample.int(p, p_bin) else integer(0)
      Xmix <- Z
      if (length(idx_bin) > 0) Xmix[, idx_bin] <- (Z[, idx_bin] > 0) * 1.0
      Xmix
    },
    stop("Unknown x$family: ", family)
  )

  # 3) zero inflation overlay (applies to all families in this first pass)
  zi <- xspec$zero_inflation %||% 0
  if (zi > 0) {
    mask <- matrix(stats::runif(n * p) < zi, nrow = n)
    X[mask] <- 0
  }

  # 4) optional standardization
  if (isTRUE(xspec$standardize)) {
    cm <- colMeans(X)
    cs <- apply(X, 2, stats::sd)
    cs[cs == 0] <- 1
    X <- sweep(X, 2, cm, "-")
    X <- sweep(X, 2, cs, "/")
  }

  X
}
```

# Working Model

Things to add

-   Different number of predictors per outcome event

-   Random vs preset predictors

-   Clinical predictors

-   Different distributions of the betas

```{r}
sim_generate_truth <- function(trspec, p, seed){
  
  # Number of active predictors
  if(trspec$sparsity < 1){
    s <- ceiling(p * (trspec$sparsity %||% 0.05))
    s <- max(0L, min(p, as.integer(s)))
  }else if(trspec$sparsity >= 1 & trspec$sparsity <= p ){
    s <- trspec$sparsity
  }
  
  #print(s)
  
  # Choice of active predictors
  if(!is.null(seed)){
    set.seed(seed)
  }else if(is.null(seed)){
    set.seed(9134)
  }
  active_predictors <- sample(1:p, 2*s-trspec$share_active_set)
  active_predictors_cause1 <- active_predictors[1:s]
  active_predictors_cause2 <- active_predictors[((s-trspec$share_active_set)+1):length(active_predictors)]
  shared_predictors <- dplyr::intersect(active_predictors_cause1,
                                        active_predictors_cause2)
  
  #rm(.Random.seed)
  
  ret <- list(shared_multiplier = trspec$shared_multiplier,
              cause1 = active_predictors_cause1,
              cause2 = active_predictors_cause2,
              shared_predictors = shared_predictors,
              share_active_set = trspec$share_active_set,
              sparsity = trspec$sparsity,
              target_eta_sd = trspec$target_eta_sd,
              beta = trspec$beta)
  
  return(ret)
  
}
```

```{r}
sim_compute_effects <- function(X, truth){
  # Simulate event of interest first (ignore masking issue)
  X1 <- X[,truth$cause1]

  #print(truth$cause1)
  #print(truth$cause2)
  #print(truth$share_active_set)
  #print(truth$shared_predictors)

  target_beta_sd <- truth$beta$sd[1]
  target_eta_sd <- truth$target_eta_sd[1]
  p1 <- ncol(X1)
  betas1 <- numeric(p1)

  
  betas1 <- rnorm(p1, 0, target_beta_sd)

  eta1_0 <- as.numeric(X1 %*% betas1)
  scale_factor <- target_eta_sd / sd(eta1_0)
  betas1 <- betas1 * scale_factor


  eta1 <- as.numeric(X1 %*% betas1)
  
  # Simulate competing event next
  X2 <- X[,truth$cause2]
  #names_X2 <- colnames(X2)
  if(exists("truth$beta$sd[2]")){
  target_beta_sd <- truth$beta$sd[2]
  }
  if(exists("truth$target_eta_sd[2]")){
  target_eta_sd <- truth$target_eta_sd[2]
  }
  p2 <- ncol(X2)
  betas2 <- numeric(p2)
  
  if(truth$share_active_set < 1){
    betas2 <- rnorm(p1, 0, target_beta_sd)
    eta2_0 <- as.numeric(X2 %*% betas2)
    scale_factor <- target_eta_sd / sd(eta2_0)
    betas2 <- betas2 * scale_factor
  }else{
    i1 <- match(truth$shared_predictors, truth$cause1)
    i2 <- match(truth$shared_predictors, truth$cause2)
    
    if(length(truth$shared_multiplier) == 1){
      truth$shared_multiplier == rep(truth$shared_multiplier, share_active_set)
    }
    
    #print(truth$shared_multiplier)
    #print(betas1[i1])
  
    betas2[i2] <- betas1[i1] * 1/truth$shared_multiplier
    #print(betas2[i2])
    betas2[-i2] <- rnorm(p2-length(i2), 0, target_beta_sd)
    
    eta_overlap <- as.numeric(X2[,i2, drop = FALSE] %*% betas2[i2])
    #print(eta_overlap)

    eta_nonoverlap_raw <- as.numeric(X2[, -i2, drop = FALSE] %*% betas2[-i2])
    scale_factor2 <- target_eta_sd / sd(eta_nonoverlap_raw)
    betas2[-i2] <- betas2[-i2] * scale_factor2
        #print(eta_nonoverlap_raw)
    
    # solve scale factor for nonoverlap betas such that 
    # sd(eta_overlap + scale*non_overlap) = target_sd
  #   objective <- function(c) {
  #   sd(eta_overlap + c * eta_nonoverlap_raw) - target_eta_sd
  #   }
  # 
  #   c_hat <- uniroot(objective, interval = c(-100, 100))$root
  #   betas2[-i2] <- c_hat * betas2[-i2]
   }
  
    eta2 <- as.numeric(X2 %*% betas2)
  
  ret <- list(
    effects1 = list(
      betas1 = betas1,
      betas1_sd = sd(betas1),
      eta1 = eta1,
      eta1_sd = sd(eta1)
    ),
    effects2 = list(
      betas2 = betas2,
      betas2_sd = sd(betas2),
      eta2 = eta2,
      eta2_sd = sd(eta2)
    )
  )

  return(ret)

  
}
```

# Latent Outcome Model

## Weibull proportional hazards model

Parameterization of the weibull model based off of the weibull distribution with shape parameter $\gamma > 0$ and scale/rate parameter $\lambda > 0$ . Shape controls whether the hazard increases or decreases with time. As $\lambda$ increases, the hazard increases, and survival is shorter. Note the parameterization is different here than in parameterizations which relate scale $b$ to xx, which can be achieved with the transformation $b = \lambda ^{-1/\gamma}$.

Weibull proportional hazards relationships.

Baseline hazard function

$$
h_0(t) = \gamma\lambda t^{\gamma-1}
$$

Baseline cumulative hazard function

$$
H_0(t) = \int_0^t h_0(s) ds \\
=  \lambda \int_0^t \gamma s^{\gamma-1} ds\\
= \lambda t^{\gamma}\\
= -\log(S_0(t))
$$

Baseline survival function

$$
S_0(t) = \exp(-H_0(t)) \\ 
= \exp(-\lambda t^{\gamma})\\
= 1 - F_0(t)
$$

Baseline probability density function

$$
f_0(t) = S_0(t) h_0(t)\\
=  \exp(-\lambda t^{\gamma})\gamma\lambda t^{\gamma-1}\\
= -\frac{d}{dt}S_0(t)\\
= \frac{d}{dt}F_0(t)
$$

Baseline cumulative incidence function

$$
F_0(t) = 1- S_0(t) \\
= 1 - \exp(-\lambda t^{\gamma})
$$

This cumulative incidence function is a weibull CDF.

With linear predictors given by $\eta_i = x_i^T \beta$, the person specific functions are

Person specific hazard function

$$
h_i(t) = h_0(t) \exp(\eta_i) \\
=\gamma\lambda t^{\gamma-1} \exp(\eta_i)
$$

Person specific cumulative hazard function

$$
H_i(t) = \int_0^t h_i(s) ds \\
=  \lambda \exp(\eta_i) \int_0^t \gamma s^{\gamma-1} ds\\
= \lambda t^{\gamma} \exp(\eta_i)\\
= -\log(S_it))
$$

Person specific survival function

$$
S_i(t) = \exp(-H_i(t)) \\ 
= \exp(-\lambda t^{\gamma}\exp(\eta_i))\\
= 1 - F_i(t)
$$

Person specific density function

$$
f_i(t) = S_i(t) h_i(t)\\
=  \exp(-\lambda t^{\gamma}\exp(\eta_i))\gamma\lambda t^{\gamma-1}\exp(\eta_i)\\
= -\frac{d}{dt}S_i(t)\\
= \frac{d}{dt}F_i(t)
$$

Person specific cumulative distribution function

$$
F_i(t) = 1- S_i(t) \\
= 1 - \exp(-\lambda t^{\gamma}\exp(\eta_i))
$$

The effective scale of the

### Randomness of Eta

Consider $\eta \sim N(0, \sigma^2)$ to be a random pull. Then $\exp(\eta) \sim$ lognormal. This means that only the conditional event time $T_i | \eta_i$ is weibull ditributed.

## Default values

Generic value of gamma: gamma = 1

-   Constant hazard: 1

-   Moderately increasing hazard: [1.2, 1]

-   Decreasing hazard due to early risk: \< 1

Generic values of lambda

-   Choose a median survival time $m$ so that we can solve for

    $$
    m:= arg{ S_0(m) = 0.5}\\
     -\lambda m^{\gamma} = 0.5 \\
     \lambda = \frac{log 2}{m ^{\gamma}}
    $$

-   Choose a baseline event probability $p$ at a certain time $\tau$

    $$
    p:= Pr(T_{event} \leq \tau) = 1 - S_0(\tau) \\
    S_0(\tau) = 1-p\\
    \exp(-\lambda \tau ^ {\gamma}) = 1-p \\
    \lambda = \frac{-\log(1-p)}{\tau^{\gamma}}
    $$

Generic values of sd of eta

-   Mild: 0.3 (HR increases 1.35 per standard deviation)

## Function

```{r}
sim_weibull_ph <- function(shape, scale, eta_val){
  x_frame <- data.frame(eta = eta_val)


  names(x_frame)<- "eta"
  betas_frame <- c(eta = 1)

  simsurv::simsurv(dist = "weibull",
                   lambdas = scale,
                   gammas = shape,
                   x =x_frame,
                   betas = betas_frame)
}
```

# Censoring

```{r}
sim_generate_censor_time <- function(cspec, n, maxtime){
  if(cspec$type == "none") {
    return(rep(maxtime + 10, n))
  }else if(cspec$type == "administrative") {
    return(rep(cspec$administrative_time, n))
  }
  
  random_censor_dist <- cspec$random$dist
  random_censor_admin <- cspec$administrative_time
  latent_censor_time <- switch(
    random_censor_dist,
    exponential ={rexp(n, rate = 1/(random_censor_admin+30))},
    stop("Unknown random censor distribution: ", random_censor_dist)
  )
}
```

# Getting defaults

```{r}
fpath <- system.file( "example_dat.Rdata", package = "bhCRR")

if (nzchar(fpath)) {
  load(fpath)
  # The objects are now in your environment
} else {
  warning("Could not find the dataset in the installed package. Loading from inst file.")
  
  load("../inst/example_dat.Rdata") 
}

example_dm <- example_dat |> dplyr::select(dplyr::starts_with("ASV"))
```

```{r}
scenario1 <- sim_spec (
  n = nrow(example_dm), # samples
  p = ncol(example_dm), # features
  #x = list(), # design matrix will be custom
  truth = list(share_active_set = 2,
               shared_multiplier = c(0.5, 0.2),
               sparsity = 0.05,
               beta = list(dist = "normal", mean = 0, sd = c(0.3, 0.3)),
               target_eta_sd = c(0.5, 0.5)), # variable selection
  risks = list(
      cause1 = list(
        baseline = list(dist = "weibull", shape = 1, scale = 0.011),
        link = "PH"
      ),
      cause2 = list(
        baseline = list(dist = "weibull", shape = 1, scale = 0.011),
        link = "PH"
      )), # risk 
  censor = list(type = "administrative",
                administrative_time = 65), #type of censoring
  seed = 9134
  
)


```

```{r}
scenario1_sim <- sim_generate(scenario1,
                         custom_design_matrix = example_dm,
                         custom_predictor_set = NULL,
                         custom_betas = NULL,
                         seed = 9134)
```

# Controlling Event Rates

To show that we are successfully able to control outcomes, I will generate a scenarios based on a specified shape=0 (constant hazard) and scale values (calculated for baseline probability of events at time 100) for the weibull proportional hazards distribution. I will generate 10 sets of this scenario to check whether we are 1) Getting the expected weibull distribution for the latent event times and 2) Getting the desired event 1/event 2/censoring proportion.

## Checking generated default parameters

I will fix the design matrix, the predictor set for both event 1 (of interest) and event 2 (competing risk), and the covariate effect for both events based on the default from scenario1_sim (run above).

The important predictors for event 1.

```{r}
names(example_dm[,scenario1_sim$truth$cause1])
```

The betas and corresponding hazard ratios.

```{r}
scenario1_sim$truth$effects$effects1$betas1
exp(scenario1_sim$truth$effects$effects1$betas1)
```

The distribution of the linear predictors. The standard deviation should be 0.5 as specified by the default.

```{r}
lp_event1 <- as.matrix(example_dm[,scenario1_sim$truth$cause1]) %*% scenario1_sim$truth$effects$effects1$betas1

sd(lp_event1)
```

```{r}
hist(lp_event1)
```

The important predictors for event 2.

```{r}
names(example_dm[,scenario1_sim$truth$cause2])
```

The betas and corresponding hazard ratios.

```{r}
scenario1_sim$truth$effects$effects2$betas2
exp(scenario1_sim$truth$effects$effects2$betas2)
```

The distribution of the linear predictors. The standard deviation should be near 0.5 as specified by the default, but currently trouble shooting on how to target the sd while inducing masking effects. (See commented out section on objective solver in sim_compute_effects).

```{r}

lp_event2 <- as.matrix(example_dm[,scenario1_sim$truth$cause2]) %*% scenario1_sim$truth$effects$effects2$betas2

sd(lp_event2)

```

```{r}
hist(lp_event2)
```
